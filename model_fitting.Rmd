---
title: "Model Fitting SIM project 1"
author: "Adri√† Casanova Lloveras"
date: "2023-11-15"
output: 
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  chunk_output_type: console
---


```{r}
# Clean workspace
if(!is.null(dev.list())) dev.off()
rm(list = ls())
```

```{r}
# Load libraries
library(car)
library(mice)
library(dplyr) 
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
library(corrplot)
library(MASS)
library(effects)
```


```{r}
# Load data
df = read.csv("train_impute.csv")

# Declare factors
df$OverallQual <- as.factor(df$OverallQual)
df$MSSubClass <- as.factor(df$MSSubClass)
char_var <- which(sapply(df, is.character))
df[,char_var] <- lapply(df[, char_var], as.factor)
```


# Create first model with all variables

```{r}
df_num <- df[, which(sapply(df, is.numeric))]
m0 = lm(SalePrice ~ ., data=df_num)

summary(m0)

vif(m0)
# The X1stFlrSF, X2ndFlrSF, GrLivArea, has a vif correlation bigger than 10, indicating that we'll need to do some kind of transformation or simply remove it.
```


```{r}
# Let's store the indices of the variables with at least one star in the lm and vif<5
id_num_star1 = c(1:5,15,17,21:23)
df_num1 <- df_num[, id_num_star1]
m1 = lm(SalePrice ~., data=df_num1)
summary(m1)
vif(m1)
corr_mat <- cor(df_num1)
corrplot(corr_mat)
cor.test(df_num1$YearBuilt, df_num1$YearRemodAdd)
corr_mat
```

YearBuilt and YearRemodAdd are highly correlated and YearBuilt is more correlated with the target. Hence,we remove YearRemodAdd in the next model

```{r}
id_num_star2 = c(1:3,5,15,17,21:23)
df_num2 <- df_num[, id_num_star2]
m2 = lm(SalePrice ~., data=df_num2)
summary(m2)
vif(m2)
corr_mat <- cor(df_num2)
corrplot(corr_mat)
corr_mat
```

Now, the most correlated variables in our model have at most a coefficient of correlation of 0.315, which in the context of real estate it is weak. We have obtained this information from https://37parallel.com/real-estate-correlation/.

```{r}
Anova(m2)
```

Anova shows that all the variables we have kept are relevant.

Let's test the model. Later, we will study the possible transformations of the variables

```{r}
plot(m2)
```

```{r}
influencePlot(m2)
# D's threshold
D_thresh <- 2/sqrt(dim(df_num2)[1]); D_thresh
```

There aren't outliers in the model according to the Cook's distance.


```{r}
step(m2)
```

Errors aren't normally distributed, so we will use boxTidwell() to test whether some transformation of the variables should be carried.
```{r}
#residualPlots(m2)
#avPlots(m2)
#crPlot(m2)
```


TRANSFORMATIONS OF VARIABLES

boxcox
```{r}
boxcox(m2)
# We should apply log(x) to SalePrice
m3 = lm(log(SalePrice)~., data=df_num2)
summary(m3)
```

Ajusted R-squared has increased about 4%.

```{r}
par(mfrow=c(2,2))
plot(m3, id.n=5)
```


```{r}
#boxTidwell(SalePrice ~ ., data=df_num2)   THIS GIVES ERROR because most variables have null values
# We'll assign 10^(-6) to all cells equal to 0 to be able to use boxTidwell without alterating too much the model

df_num2 = replace(df_num2, df_num2 == 0, 1e-6)
#for (i in 1:(length(names(df_num2))-1)) {
#  df_num2[which(df_num2[,i]==0),i] = 1e-12
#}
summary(df_num2)
boxTidwell(SalePrice~., data=df_num2)



```

