---
title: "Model Fitting SIM project 1"
author: "Adri√† Casanova Lloveras"
date: "2023-11-15"
output: 
  pdf_document: 
    toc: true
    toc_depth: 3
    number_sections: true
editor_options: 
  chunk_output_type: console
---

```{r}
# Clean workspace
if(!is.null(dev.list())) dev.off()
rm(list = ls())
```

```{r}
# Load libraries
library(car)
library(mice)
library(dplyr) 
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
library(corrplot)
library(MASS)
library(effects)
```


```{r}
# Load data
df = read.csv("train_impute.csv")

# Declare factors
df$OverallQual <- as.factor(df$OverallQual)
df$MSSubClass <- as.factor(df$MSSubClass)
char_var <- which(sapply(df, is.character))
df[,char_var] <- lapply(df[, char_var], as.factor)
```


# Create first model with all variables

```{r}
df_num <- df[, which(sapply(df, is.numeric))]
m0 = lm(SalePrice ~ ., data=df_num)

summary(m0)

vif(m0)
# The X1stFlrSF, X2ndFlrSF, GrLivArea, has a vif correlation bigger than 10, indicating that we'll need to do some kind of transformation or simply remove it.
```


```{r}
# Let's store the indices of the variables with at least one star in the lm and vif<5
id_num_star1 = c(1:5,15,17,21:23)
df_num1 <- df_num[, id_num_star1]
m1 = lm(SalePrice ~., data=df_num1)
summary(m1)
vif(m1)
corr_mat <- cor(df_num1)
corrplot(corr_mat)
cor.test(df_num1$YearBuilt, df_num1$YearRemodAdd)
corr_mat
```

YearBuilt and YearRemodAdd are highly correlated and YearBuilt is more correlated with the target. Hence,we remove YearRemodAdd in the next model

```{r}
id_num_star2 = c(1:3,5,15,17,21:23)
df_num2 <- df_num[, id_num_star2]
m2 = lm(SalePrice ~., data=df_num2)
summary(m2)
vif(m2)
corr_mat <- cor(df_num2)
corrplot(corr_mat)
corr_mat
```

Now, the most correlated variables in our model have at most a coefficient of correlation of 0.315, which in the context of real estate it is weak. We have obtained this information from https://37parallel.com/real-estate-correlation/.

```{r}
Anova(m2)
```

Anova shows that all the variables we have kept are relevant.

# Model analysis
Let's test the model. Later, we will study the possible transformations of the variables

```{r}
plot(m2)
```

And we analyse if there are influent data and found that there are 3 observations that with a bigger Cook's distance than the threshold (considered as 2 / sqrt(n)). Consequently, we decided to remove those observations.

```{r}
influencePlot(m2)

# Calculate D's threshold
D_thresh <- 2/sqrt(dim(df_num2)[1]); D_thresh

#Remove the points and fit the model again
influent <- c(1183, 692, 186)

df <- df[-influent,]
df_num <- df[, which(sapply(df, is.numeric))]
df_num2 <- df_num[, id_num_star2]
m2 = lm(SalePrice ~., data=df_num2)
influencePlot(m2)
```


```{r}
step(m2)
Anova(m2)
```

Errors aren't normally distributed, so we will use boxTidwell() to test whether some transformation of the variables should be carried.
```{r}
#residualPlots(m2)
#avPlots(m2)
#crPlot(m2)
```


TRANSFORMATIONS OF VARIABLES

boxcox
```{r}
boxcox(m2)
# We should apply log(x) to SalePrice
m3 = lm(log(SalePrice)~., data=df_num2)
summary(m3)
```

Ajusted R-squared has increased about 4%.

```{r}
par(mfrow=c(2,2))
plot(m3, id.n=5)
```
The residuals' distribution is closer to normal now. However, it isn't normal yet.

```{r}
#boxTidwell(SalePrice ~ ., data=df_num2)   THIS GIVES ERROR because most variables have null values
# We'll assign 10^(-6) to all cells equal to 0 to be able to use boxTidwell without alterating too much the model

df_num2 = replace(df_num2, df_num2 == 0, 1e-6)
summary(df_num2)

# boxTidwell(log(SalePrice)~., data=df_num2) THIS GIVES ERROR AS WELL because the model has too many variables

boxTidwell(log(SalePrice) ~ LotArea+YearBuilt+MasVnrArea, data = df_num2)
# We should apply sqrt(LotArea). The lambda for YearBuilt is too large, so it would difficult to interpet the model using it. MasVnrArea has a too large p-value, so we cannot reject the null hypothesis that lambda = 1.
boxTidwell(log(SalePrice)~LotFrontage, data = df_num2)
# Too small lambda
boxTidwell(log(SalePrice)~BedroomAbvGr, data = df_num2)
# Too large p-value
boxTidwell(log(SalePrice)~Fireplaces, data =df_num2)
# We apply log() to Fireplaces
boxTidwell(log(SalePrice)~WoodDeckSF, data = df_num2)
# We apply sqrt() to WoodDeckSF
boxTidwell(log(SalePrice)~OpenPorchSF, data = df_num2)
# Too small lambda
```



```{r}
m4 = lm(log(SalePrice) ~ LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+log(Fireplaces)+sqrt(WoodDeckSF)+OpenPorchSF,
        data=df_num2)
summary(m4)
plot(m4)
```
Adjusted R-squared has increased slightly. Since we cannot find a significant improvement, we will compare m3 and m4 with more advanced tools.

```{r}
AIC(m3, m4)
BIC(m3, m4)
```
m3 has slightly lower AIC and BIC, so actually it is  better than m4.

We'll try now to only apply a few of the relevant transformations.

```{r}
m5 = lm(log(SalePrice) ~ LotFrontage+LotArea+YearBuilt+MasVnrArea+
          BedroomAbvGr+log(Fireplaces)+sqrt(WoodDeckSF)+OpenPorchSF,
        data=df_num2)
summary(m5)
AIC(m4,m5)
BIC(m4,m5)
```

```{r}
m6 = lm(log(SalePrice) ~ LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF,
        data=df_num2)
summary(m6)
AIC(m4,m6)
BIC(m4,m6)
```

```{r}
m7 = lm(log(SalePrice) ~ LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+log(Fireplaces)+WoodDeckSF+OpenPorchSF,
        data=df_num2)
summary(m7)
AIC(m4,m7)
BIC(m4,m7)
```

```{r}
m8 = lm(log(SalePrice)~LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea
        +BedroomAbvGr+Fireplaces+WoodDeckSF+OpenPorchSF, 
        data=df_num2); summary(m8)

m9 = lm(log(SalePrice)~LotFrontage+LotArea+YearBuilt+MasVnrArea
        +BedroomAbvGr+log(Fireplaces)+WoodDeckSF+OpenPorchSF, 
        data=df_num2); summary(m9)

m10 = lm(log(SalePrice)~LotFrontage+LotArea+YearBuilt+MasVnrArea
        +BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF, 
        data=df_num2); summary(m10)
AIC(m4,m10)
```

The best model is m6, that only applies sqrt transformations in LotArea and WoodDeckSF. For this model we have compared the distribution of residuals and saw that it was very similar to the original model


```{r}
m11 = lm(log(SalePrice) ~ LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF,
        data=df_num)
BIC(m3,m11)
plot(m11)
```

# Factors
As there were an important number of numeric variables we tried to add factor variables one by one. We started with the most correlated variable with the target and continued in decreasing order. To test if they increase the model forecasting capability we analysed the BIC and the distribution of residuals.

For example, we added "ExterQual" and saw that it increased R^2 while reducing the BIC. Moreover, Anova and step methods suggested that adding the variable was relevant.


```{r}
#OverallQual, ExterQual, BsmtQual, KitchenQual, Neighborhood, GarageFinish  
#FireplaceQu, Foundation, GarageType and MSSubClass
m12 = lm(log(SalePrice)~LotFrontage+sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual, 
        data=df); summary(m12)
BIC(m10,m12)
Anova(m12)
step(m12, k = log(nrow(df)))
```

```{r}
m12.1 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual, 
        data=df); summary(m12.1)
BIC(m10,m12,m12.1)
```


```{r}
m13 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual+ExterQual, 
        data=df); summary(m13)
BIC(m13,m12.1,m11)
Anova(m13)
step(m13, k = log(nrow(df)))
```

```{r}
m14 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual+ExterQual+BsmtQual, 
        data=df); summary(m14)
BIC(m14,m13,m12.1,m12,m11)
Anova(m14)
step(m14, k = log(nrow(df)))
```

```{r}
m15 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual+ExterQual+BsmtQual+KitchenQual, 
        data=df); summary(m15)
BIC(m15,m14,m13,m12.1,m12,m11)
Anova(m15)
step(m15, k = log(nrow(df)))
```

```{r}
m15.1 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual+BsmtQual+KitchenQual, 
        data=df); summary(m15)
BIC(m15.1,m15,m14,m13,m12.1,m12,m11)
Anova(m15.1)
step(m15.1, k = log(nrow(df)))
```

```{r}
m16 = lm(log(SalePrice)~sqrt(LotArea)+YearBuilt+MasVnrArea+
          BedroomAbvGr+Fireplaces+sqrt(WoodDeckSF)+OpenPorchSF+OverallQual+BsmtQual+KitchenQual+Neighborhood, data=df); summary(m15)
BIC(m16,m15.1,m15,m14,m13,m12.1,m12,m11)
Anova(m16)
step(m16, k = log(nrow(df)))
```

m16 has a lower BIC than m15.1, so we keep m15.1 and stop adding categories.


# Model selection
From the resulting model, we tried to reduce the number of variables using "step" method with BIC (Bayesian information criteria) to consider a greater effect of the complexity on the model considering that there are a lot of variables. Additionally we performed an "Anova" method, that compares a base model with a nested model of one less variable.

Both methods suggested that the best possible model is obtained without removing any variable. For example, in AIC, removing "BedroomAbvGr" makes a simpler model but it is less predictive. As a consequence BIC is increased from 31053 to 31056. In "Anova" the p-value for this value is not significant and, as a consequence, we cannot remove this variable as the resulting model is not equivalent.


```{r}
# we use k = log(n) to use BIC instead of AIC 
step(m2, k = log(nrow(df))) 
Anova(m0)
```
