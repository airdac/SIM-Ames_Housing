abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7,13))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7,8), ylim = c(400,600))
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7.4,8), ylim = c(400,600))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7.4,8), ylim = c(400,550))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7.45,7.9), ylim = c(420,530))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 0.5)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 10)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 10)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 5)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7,13))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 5)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 3)
res.mout <- Moutlier(test_impute[,c(3,5,7,16,17,19,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7,13))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(df), cex = 3)
text(res.mout$md, res.mout$rd, label = row.names(test), cex = 3)
res.mout <- Moutlier(test_impute[,c(3,5,7,16,17,19,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19, xlim = c(7,13))
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test), cex = 3)
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test), cex = 3)
ll <- which ((res.mout$md>res.mout$cutoff) &
res.mout$rd>res.mout$cutoff); ll
text(res.mout$md, res.mout$rd, label = row.names(test), cex = 1)
res.mout <- Moutlier(test_impute[,c(3,5,7,16,17,19,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test), cex = 1)
# We can observe that there is quite a lot severe outliers.
severe_out
# Find the numerical sets and categorical sets
test_num = select(test, Numerical_val); summary(test_num)
# Find the numerical sets and categorical sets
test_num = select(test, Numerical_val_2); summary(test_num)
cat_num = select(test, Categorical_val); summary(cat_num)
# Here we'll check all the Univariate outlier, and highlight them. In the next section, we'll do it but for multiple outlier. If the record appear in both Uni&Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(select(test_num,Numerical_val_2))
# Here we'll check all the Univariate outlier, and highlight them. In the next section, we'll do it but for multiple outlier. If the record appear in both Uni&Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(test_num)
res.mout <- Moutlier(test_num[,c(3,5,7,16,17,19,21)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(2)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(3)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(4)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(5)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(6)], quantile = 0.95)
test_num
res.mout <- Moutlier(test_num], quantile = 0.95)
res.mout <- Moutlier(test_num, quantile = 0.95)
# Find the numerical sets and categorical sets
test_num = select(test, Numerical_val_2); summary(test_num)
res.mout <- Moutlier(test_num, quantile = 0.95)
par(mfrow = c(1,1))
res.mout <- Moutlier(test_num[1], quantile = 0.95)
res.mout <- Moutlier(test_num[1,2], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,2)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,3)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,4)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,5)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,6)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,6)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,8)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,9)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,9)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,10)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,11)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,12)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,13)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,14)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,15)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,17)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,17)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,17)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,18)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,19)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,20)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21,22)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21,23)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21,24)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21,25)], quantile = 0.95)
res.mout <- Moutlier(test_num[,c(1,7,16,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test_num), cex = 1)
res.mout <- Moutlier(test_num[,c(1,7,16)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test_num), cex = 1)
ll <- which ((res.mout$md>res.mout$cutoff) & res.mout$rd>res.mout$cutoff); ll
res.mout <- Moutlier(test_num[,c(1,7,16,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test_num), cex = 1)
ll <- which ((res.mout$md>res.mout$cutoff) & res.mout$rd>res.mout$cutoff); ll
ll
test_clean = test_num[,-ll]
test_clean
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout <- Moutlier(test_clean[,c(1,7,16,21)], quantile = 0.95)
# In the multivariate outlier, as this is stochastic, we cannot plot all the features. After several attempts, the best combination is below
res.mout <- Moutlier(test_num[,c(1,7,16,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test_num), cex = 1)
ll <- which ((res.mout$md>res.mout$cutoff) & res.mout$rd>res.mout$cutoff); ll
# So we'll remove them from the test data set.
test_clean = test_num[,-ll]
res.PCA = imputePCA (test_clean)   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
test_impute = res.PCA$completeObs; summary(test_impute)
test_clean = test_num[-ll,]
res.PCA = imputePCA (test_clean)   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
test_impute = res.PCA$completeObs; summary(test_impute)
# With the summary, we can see that there are 80 variables in total.
summary(train)
# And the name of each feature are below.
str(train)
# Load the data
if(!is.null(dev.list())) dev.off()
rm(list = ls())
train = read.csv("train.csv")
test = read.csv("test.csv")
# And the combination of both dataset are:
df = rbind(test, train[,-81])
# Import the necessary library
library(car)
library(mice)
library(dplyr)
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
source("LittleMCAR function.R")
# With the summary, we can see that there are 80 variables in total.
summary(train)
# And the name of each feature are below.
str(train)
Categorical_val = c('MSSubClass',"MSZoning","Street","Alley","LotShape","LandContour","Utilities","LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType","HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st","Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC","CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature","SaleType","SaleCondition", "LowQualFinSF", "PoolArea")
Numerical_val = c("LotFrontage","LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","MiscVal")
Numerical_val_2 = c("LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","MiscVal")
Date_val = c("YearBuilt","YearRemodAdd","GarageYrBlt","MoSold","YrSold")
# Some numerical variable just contain few unique values, which means can be converted into categorical. Here below we can see which of them.
sapply(select(df, Numerical_val), table)
sapply(select(df, Categorical_val), table)
sapply(select(df, Date_val), table)
# Some numerical variable just contain few unique values, which means can be converted into categorical. Here below we can see which of them.
sapply(select(train, Numerical_val), table)
sapply(select(train, Categorical_val), table)
sapply(select(train, Date_val), table)
summary(train$LotFrontage)
summary(train$PoolArea)
# Some numerical variable just contain few unique values, which means can be converted into categorical. Here below we can see which of them.
sapply(select(train, Numerical_val), table)
sapply(select(train, Categorical_val), table)
# Assumption 1
test <- test %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
test$PoolArea = as.factor(test$PoolArea)
train <- train %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
train$PoolArea = as.factor(train$PoolArea)
# Assumption 2
test <- test %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
test$LowQualFinSF = as.factor(test$LowQualFinSF)
train <- train %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
train$LowQualFinSF = as.factor(train$LowQualFinSF)
# Assumption 3
test <- subset(test, select = -LotFrontage)
train <- subset(train, select = -LotFrontage)
# Assumption 4
breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
labels = c("Very Poor","Poor","Fair","Below Average","Average", "Above Averager", "Good", "Very Good", "Excellent", "Very Excellent")
test$OverallQual <- cut(test$OverallQual, breaks = breaks, labels = labels, include.lowest = TRUE)
test$OverallCond <- cut(test$OverallCond, breaks = breaks, labels = labels, include.lowest = TRUE)
train$OverallQual <- cut(train$OverallQual, breaks = breaks, labels = labels, include.lowest = TRUE)
train$OverallCond <- cut(train$OverallCond, breaks = breaks, labels = labels, include.lowest = TRUE)
test$MSSubClass = as.factor(test$MSSubClass)
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
train$MSSubClass = as.factor(train$MSSubClass)
train$OverallQual = as.factor(train$OverallQual)
train$OverallCond = as.factor(train$OverallCond)
# Assumption 5.
names(test)[names(test) == "Brk Cmn"] <- "BrkComm"
# Assumption 6.
test <- test %>%
mutate_if(is.character, as.factor)
train <- train %>%
mutate_if(is.character, as.factor)
# Assumption 7.
test$MoSold = month.name[test$MoSold]
test$MoSold = as.factor(test$MoSold)
train$MoSold = month.name[train$MoSold]
train$MoSold = as.factor(train$MoSold)
summary(test)
# Find the numerical sets and categorical sets
train_num = select(train, Numerical_val_2); summary(train_num)
train_cat = select(train, Categorical_val); summary(train_cat)
# Here we'll check all the Univariate outlier, and highlight them. In the next section, we'll do it but for multiple outlier. If the record appear in both Uni&Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(train_num)
# We can observe that there is quite a lot severe outliers.
severe_out
res.mout <- Moutlier(train_num, quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,2)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,2,3)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(2,3)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(2)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(3)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(4)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(5)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(6)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(7)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(8)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(9)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(10)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,2,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,4,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,6,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,6,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,8,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,9,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,10,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,11,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,12,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,13,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,14,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,15,16,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,18,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,,2021)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,,20,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,20,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,21,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,22,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,23,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,24,21)], quantile = 0.95)
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(test_num), cex = 1)
text(res.mout$md, res.mout$rd, label = row.names(train_num), cex = 1)
ll <- which ((res.mout$md>res.mout$cutoff) & res.mout$rd>res.mout$cutoff); ll
# So we'll remove them from the test data set.
test_clean = test_num[-ll,]
# So we'll remove them from the test data set.
test_clean = train_num[-ll,]
res.PCA = imputePCA (train_clean)   # impute numeric variables
str (res.PCA)
# Import the necessary library
library(car)
library(mice)
library(dplyr)
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
source("LittleMCAR function.R")
res.PCA = imputePCA (train_clean)   # impute numeric variables
# So we'll remove them from the test data set.
train_clean = train_num[-ll,]
# PCA imputation
```{r}
res.PCA = imputePCA (train_clean)   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
test_impute = res.PCA$completeObs; summary(test_impute)
# We can observe that the missing values are imputed.
train_impute = res.PCA$completeObs; summary(test_impute)
# We can observe that the missing values are imputed.
train_impute = res.PCA$completeObs; summary(train_impute)
condes(train, 81)
condes(train, 81)
# We can observe that the missing values are imputed.
train_impute = res.PCA$completeObs; summary(train_impute)
# Load the data
if(!is.null(dev.list())) dev.off()
rm(list = ls())
train = read.csv("train.csv")
test = read.csv("test.csv")
# And the combination of both dataset are:
df = rbind(test, train[,-81])
condes(train, "SalePrice")
condes(train, 81)
# Chunk 1
# Load the data
if(!is.null(dev.list())) dev.off()
rm(list = ls())
train = read.csv("train.csv")
test = read.csv("test.csv")
# And the combination of both dataset are:
df = rbind(test, train[,-81])
# Chunk 2
# Import the necessary library
library(car)
library(mice)
library(dplyr)
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
source("LittleMCAR function.R")
# Chunk 3
# With the summary, we can see that there are 80 variables in total.
summary(train)
# And the name of each feature are below.
str(train)
# Analyzing all the feature will be an exhausting work. So there should be some way to reduce the dimensional. According to the statement of this project, we should retain all the numerical variable and 10 categorical variable.
# The categorical variable are below
Categorical_val = c('MSSubClass',"MSZoning","Street","Alley","LotShape","LandContour","Utilities","LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType","HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st","Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC","CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature","SaleType","SaleCondition", "LowQualFinSF", "PoolArea")
# The numerical variable are
Numerical_val = c("LotFrontage","LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","MiscVal")
Numerical_val_2 = c("LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","MiscVal")
# And the date variable are
Date_val = c("YearBuilt","YearRemodAdd","GarageYrBlt","MoSold","YrSold")
# Chunk 4
# Some numerical variable just contain few unique values, which means can be converted into categorical. Here below we can see which of them.
sapply(select(train, Numerical_val), table)
sapply(select(train, Categorical_val), table)
sapply(select(train, Date_val), table)
# Chunk 5
# The LotFrontage has a percentage of missing quite high, 17.75% (259/1459). And it's the measure of distance from the property to the street. We'll need to check if it's missing by random or not, and decide to keep it or remove it.
# For this purpose, we'll use the Little test to validade if it's a MCAR, MAR or MNA.
# little.test <- LittleMCAR(train)
# NO ES POT FER SERVIR LITTLE TEST PERQUE ELS VARIABLES HA SUPERAT ELS 50.
# Chunk 6
# Assumptions 1. The PoolArea, although it's a Numerical variable, the percentage recorded is only 7 out of 1459 observations (0,48% of all data). So we thinks maybe can be reduced to a binary variable (Having or Not a Pool)
# Assumption 2. The LowQualFinSF is referring to Surface finished but with low quality. As same as PoolArea, there is only 23 unique observation different than 0. So we can reduce it into binary variable (Having or not a low quality surface.)
# Assumption 3. The LotFrontage has a percentage of missing quite high, 17.75% (259/1459). And it's the measure of distance from the property to the street. Using condes function, we can see that the relation between it and the target is 0.3518, with P-value 2,6*10^-36. We can consider to remove it.
# Assumption 4. The MSSubClass, OverallQual and OverallCond, although are categorical variables, they are presented as integer. So we need to transform it into factor.
#               For the OverallQual and OverallCond variable, their level are scaled from 1 to 10. Instead of that, we'll like to switch to categorical (Very pool to very Excelent)
# Assumption 5. Checking the categorical set, we found that "Exterior2nd" has a record of "Brk Cmn", which does not match with the data description "BrkComm". So we rename it (in order to match with "Exterior1st")
# Assumption 6. For all those categorical variable that we find previously, we'll transform it into Factor.
# Assumption 7. The month will be transformed into factor and renamed to abbreviated names.While other date as year will maintain the numerical variable
# Chunk 7
# Assumption 1
test <- test %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
test$PoolArea = as.factor(test$PoolArea)
train <- train %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
train$PoolArea = as.factor(train$PoolArea)
# Assumption 2
test <- test %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
test$LowQualFinSF = as.factor(test$LowQualFinSF)
train <- train %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
train$LowQualFinSF = as.factor(train$LowQualFinSF)
# Assumption 3
test <- subset(test, select = -LotFrontage)
train <- subset(train, select = -LotFrontage)
# Assumption 4
breaks = c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)
labels = c("Very Poor","Poor","Fair","Below Average","Average", "Above Averager", "Good", "Very Good", "Excellent", "Very Excellent")
test$OverallQual <- cut(test$OverallQual, breaks = breaks, labels = labels, include.lowest = TRUE)
test$OverallCond <- cut(test$OverallCond, breaks = breaks, labels = labels, include.lowest = TRUE)
train$OverallQual <- cut(train$OverallQual, breaks = breaks, labels = labels, include.lowest = TRUE)
train$OverallCond <- cut(train$OverallCond, breaks = breaks, labels = labels, include.lowest = TRUE)
test$MSSubClass = as.factor(test$MSSubClass)
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
train$MSSubClass = as.factor(train$MSSubClass)
train$OverallQual = as.factor(train$OverallQual)
train$OverallCond = as.factor(train$OverallCond)
# Assumption 5.
names(test)[names(test) == "Brk Cmn"] <- "BrkComm"
# Assumption 6.
test <- test %>%
mutate_if(is.character, as.factor)
train <- train %>%
mutate_if(is.character, as.factor)
# Assumption 7.
test$MoSold = month.name[test$MoSold]
test$MoSold = as.factor(test$MoSold)
train$MoSold = month.name[train$MoSold]
train$MoSold = as.factor(train$MoSold)
summary(test)
# Chunk 8
# Find the numerical sets and categorical sets
train_num = select(train, Numerical_val_2); summary(train_num)
train_cat = select(train, Categorical_val); summary(train_cat)
# Chunk 9
# Here we'll check all the Univariate outlier, and highlight them. In the next section, we'll do it but for multiple outlier. If the record appear in both Uni&Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(train_num)
# We can observe that there is quite a lot severe outliers.
severe_out
# Chunk 10
# In the multivariate outlier, as this is stochastic, we cannot plot all the features. After several attempts, the best combination is below
res.mout <- Moutlier(train_num[,c(1,3,5,7,16,17,19,21)], quantile = 0.95)
par(mfrow = c(1,1))
plot(res.mout$md, res.mout$rd, col="cyan", pch = 19)
res.mout$cutoff
abline(h=res.mout$cutoff, col="red", lwd = 2)
abline(v=res.mout$cutoff, col="red", lwd = 2)
text(res.mout$md, res.mout$rd, label = row.names(train_num), cex = 1)
ll <- which ((res.mout$md>res.mout$cutoff) & res.mout$rd>res.mout$cutoff); ll
# So we'll remove them from the test data set.
train_clean = train_num[-ll,]
# Chunk 11
# In this section we'll Performance PCA to impute the missing data for numerical variables
res.PCA = imputePCA (train_clean)   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
train_impute = res.PCA$completeObs; summary(train_impute)
# Chunk 12
# When we have set the correct type of each variable we can start to analyse the database. We can do a quick EDA on both databases using automatic tools like "DataExplorer".
# Using those reports we see that databases "test" and "train" have the same variable (80) except for "SalePrice", that can only be found in "train". In more detail, we can see that "train" db contains the firsts 1460 observations, of houses that were sold, and "test" contains the next 1459 observations of houses in which we don't know the price that were sold. We will work to predict the price of those houses.
# Chunk 13
# load the DataExplorer library
# use create_report
# create_report(train)
# create_report(test)
# Chunk 14
# Categorical feature selection
# Once we have the data clean and preprocesed, we'll get into the selection of the categorical feature, leaving only with 10 of then.
# Because there are too much qualitative variables we need to select 10 of them. Note that qualitative variables in both datasets are the same but, according to the EDA, distributions are different. As a consequence, we expect correlations to be different as well.
# To analyse which categorical variables are more relevant we performed a profiling using FactoMiner. Analysing only "train" database and using "priceSold" as our target variable have analysed the 10 more important categorical variables to predict the price. Those are:
"
Variable-R^2-p value
Neighborhood  0.545574991 1.558600e-225
ExterQual     0.477387778 1.439551e-204
BsmtQual      0.464993778 8.158548e-196
KitchenQual   0.456598624 3.032213e-192
GarageFinish  0.305873737 6.228747e-115
FireplaceQu   0.293960754 2.971217e-107
Foundation    0.256368402  5.791895e-91
GarageType    0.249204231  6.117026e-87
BsmtFinType1  0.210810410  2.386358e-71
HeatingQC     0.195500486  2.667062e-67
"
# Use condes method to determinate the correlation between categorical feature and the target SalePrice.
res.con = condes(train, 80)
res.con$quanti
res.con$quali
res.cat=catdes(train, 13) # df = columns without missings
#res.cat$test.chi2
res.cat$category
res.cat=catdes(train, 80) # df = columns without missings
res.cat=catdes(train, 17) # df = columns without missings
#res.cat$test.chi2
res.cat$category
