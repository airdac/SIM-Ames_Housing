id_num_val = which(sapply(test, is.numeric)==TRUE)
# We won't analyze the id variable
id_num_val = as.numeric(id_num_val)[-1]; id_num_val
id_cat_val = which(sapply(test, is.factor)==TRUE)
id_cat_val = as.numeric(id_cat_val); id_cat_val
id_date_val = c(20,21,60,77,78)
# Now, the categorical variable are below
Categorical_val = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities","LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType","HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st","Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","BsmtFinSF2","Heating","HeatingQC","CentralAir","Electrical","LowQualFinSF","BsmtHalfBath","KitchenAbvGr","KitchenQual","Functional","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PavedDrive","EnclosedPorch","X3SsnPorch","ScreenPorch","PoolArea","PoolQC","Fence","MiscFeature","SaleType","SaleCondition","MoSold")
# The numerical variables except the target are
Numerical_val = c("LotFrontage","LotArea","YearBuilt","YearRemodAdd","MasVnrArea","BsmtFinSF1","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","FullBath","HalfBath","BedroomAbvGr","TotRmsAbvGrd","Fireplaces","GarageYrBlt","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","YrSold")
train_num = select(train, Numerical_val); summary(train_num)
train_cat = select(train, Categorical_val); summary(train_cat)
# Chunk 8
# Here we'll check all the Univariate outliers, and highlight them. In the next section, we'll do it but for multiple outliers. If a record appears in Uni or Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(train_num); severe_out
# There are 26 numerical variables, so we will automatize this step by erasing all severe univariate outliers and setting them to NA. After validating the results, we will imputate them with the rest of missing values. There will be one exception, SalePrice. Since it is the target, we can't impute it, so we will remove the whole records with severe outliers instead.
severe_outliers <- function(data) {
ss <- summary(data)
# Upper/lower severe thresholds
utso <- as.numeric(ss[5]+3*(ss[5]-ss[2]))
ltso <- as.numeric(ss[2]-3*(ss[5]-ss[2]))
return (which((data>utso)|(data<ltso)))
}
par(mfrow=c(1,2))
for (var in id_num_val) {
train[severe_outliers(train[,var]),var] <- NA
Boxplot(train[,var], ylab = var, main = "Train")
test[severe_outliers(test[,var]),var] <- NA
Boxplot(test[,var], ylab = var, main = "Test")
}
par(mfrow=c(1,1))
train = train[-severe_outliers(train$SalePrice),]
Boxplot(train$SalePrice)
# Chunk 9
# In this section we'll Performance PCA to impute the missing data for numerical variables
res.PCA = imputePCA (train[,id_num_val])   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
train_impute = data.frame(res.PCA$completeObs)
# Chunk 10
# Check if the imputation was successful or not
before_imputation = summary(train[,id_num_val])
after_imputation = summary(train_impute)
label = c('Before imputation', 'After imputation')
for (x in c(1,2,5,6,8,9,11,15,16,18,20,21,22)) {
d = data.frame(A = train[,id_num_val][x], B = train_impute[,x])
b = boxplot(d, names=label, main = names(train[,id_num_val][x]));
b
}
# Chunk 11
# In the multivariate outlier, as this is stochastic, we cannot plot all the features. After several attempts, the best combination is below
id_num_val
id_num_val_not_corr = c(1, 2, 4, 6, 7, 11, 17,
18, 20)
# The analysis gives that all values from classical Mahalanobis distance
# are NA, as a consequence it is only possible to plot robust Mahalanobis distance.
# According to this distance there are 436 outliers (30 % of all data), however,
# none of them is considered an outlier by the classical Mahalanobis distance.
res.mout <- Moutlier(train_impute[,id_num_val_not_corr], quantile = 0.95, plot= FALSE)
par(mfrow=c(1,2))
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff)
length(outliers)
res.con$quali[:10]
res.con$quali[1:10]
# Use condes() method to determine the correlation between categorical feature and the target SalePrice.
res.con = condes(train, 80)
res.con$quanti
res.con$quali
res.con$category
res.con$quali[:10]
res.con$quali[1:10]
res.con$quali[1:10,]
res.con$quali[:10,]
res.con$quali[1:10,]
cor.test(train$YrSold, train$SalePrice)
res.con$quali[1:10,]
# load DataExplorer library
#use create_report
library(DataExplorer)
#create_report(train)
#create_report(train_impute)
library(SmartEDA)
ExpReport(train, op_file = 'train_before_imputation_SMARTEDA.html')
# Check if the imputation was successful or not
before_imputation = summary(train[,id_num_val])
after_imputation = summary(train_impute)
label = c('Before imputation', 'After imputation')
for (x in c(1,2,5,6,8,9,11,15,16,18,20,21,22)) {
d = data.frame(A = train[,id_num_val][x], B = train_impute[,x])
b = boxplot(d, names=label, main = names(train[,id_num_val][x]));
b
}
# In this section we'll Performance PCA to impute the missing data for numerical variables
res.PCA.test = imputePCA (test[,id_num_val])   # impute numeric variables
str (res.PCA.test)
str(res.PCA.test$completeObs)
# We can observe that the missing values are imputed.
test_impute = data.frame(res.PCA.test$completeObs)
# Check if the imputation was successful or not
before_imputation_test = summary(test[,id_num_val])
after_imputation_test = summary(test_impute)
label = c('Before imputation', 'After imputation')
for (x in c(1,2,5,6,8,9,11,15,16,18,20,21,22)) {
d = data.frame(A = test[,id_num_val][x], B = test_impute[,x])
b = boxplot(d, names=label, main = names(test[,id_num_val][x]));
b
}
res.mout.test <- Moutlier(test_impute[,id_num_val_not_corr], quantile = 0.95, plot= FALSE)
par(mfrow=c(1,2))
plot(res.mout.test$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout.test$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout.test$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout.test$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))
outliers.test = which(res.mout.test$md>res.mout.test$cutoff & res.mout.test$rd > res.mout.test$cutoff)
length(outliers.test)
# In the multivariate outlier, as this is stochastic, we cannot plot all the features. After several attempts, the best combination is below
id_num_val
id_num_val_not_corr = c(1, 2, 4, 6, 7, 11, 17,
18, 20)
# The analysis gives that all values from classical Mahalanobis distance
# are NA, as a consequence it is only possible to plot robust Mahalanobis distance.
# According to this distance there are 436 outliers (30 % of all data), however,
# none of them is considered an outlier by the classical Mahalanobis distance.
res.mout <- Moutlier(train_impute[,id_num_val_not_corr], quantile = 0.95, plot= FALSE)
par(mfrow=c(1,2))
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff)
length(outliers)
percent_miss(train$GarageYrBlt)
percent_miss(train$GarageYrBlt)
percent_miss(test$GarageYrBlt)
percent_miss(train$MasVnrArea)
percent_miss(test$MasVnrArea)
percent_miss(train$OpenPorchSF)
percent_miss(test$OpenPorchSF)
# Chunk 1
# Load the data
if(!is.null(dev.list())) dev.off()
rm(list = ls())
train = read.csv("train.csv")
test = read.csv("test.csv")
# And the combination of both datasets is:
df = rbind(test, train[,-81])
# Chunk 2
# Import the necessary library
library(car)
library(mice)
library(dplyr)
library(missMDA)
library(FactoMineR)
library(chemometrics)
library(DataExplorer)
library(corrplot)
# Chunk 3
# With the summary, we can see that there are 80 variables in total.
summary(train)
# And the name of each feature are below.
str(train)
# Analyzing all features will be an exhausting work. So there should be some way to reduce the dimensions. According to the statement of the project, we should retain all numerical variables and 10 categorical variables.
# The categorical variable are below
Categorical_val = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities","LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType","HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st","Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","Heating","HeatingQC","CentralAir","Electrical","KitchenQual","Functional","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PavedDrive","PoolQC","Fence","MiscFeature","SaleType","SaleCondition", "MoSold")
# The numerical variables except the target are
Numerical_val = c("LotFrontage","LotArea","MasVnrArea","BsmtFinSF1","BsmtFinSF2","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","BsmtHalfBath","FullBath","HalfBath","BedroomAbvGr","KitchenAbvGr","TotRmsAbvGrd","Fireplaces","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","EnclosedPorch","X3SsnPorch","ScreenPorch","MiscVal","YearBuilt","YearRemodAdd","GarageYrBlt","YrSold")
# And the date variables are
Date_val = c("YearBuilt","YearRemodAdd","GarageYrBlt","MoSold","YrSold")
# Chunk 4
# Some numerical variables just contain a few unique values, which means they can be converted to categorical. Below we can see which of them.
sapply(select(train, Numerical_val), table)
sapply(select(train, Categorical_val), table)
sapply(select(train, Date_val), table)
# Chunk 5
# Assumptions 1. PoolArea, although it's a numerical variable, the percentage recorded is less than 0.5% in train and test. So we thinks maybe can be reduced to a binary variable (Having or Not a Pool)
length(which(train$PoolArea > 0))/dim(train)[1]*100
length(which(test$PoolArea > 0))/dim(test)[1]*100
# Assumption 2. LowQualFinSF is referring to Surface finished but with low quality. As same as PoolArea, only 2% of the values in train and 1% in test are positive. So we can reduce it into a binary variable (having or not a low quality surface.)
length(which(train$LowQualFinSF > 0))/dim(train)[1]*100
length(which(test$LowQualFinSF > 0))/dim(test)[1]*100
#Assumtption 3. The same happens with BsmtFinSF2 with only 12% of positive values.
length(which(train$BsmtFinSF2 > 0))/dim(train)[1]*100
length(which(test$BsmtFinSF2 > 0))/dim(test)[1]*100
# Assumption 4. LotFrontage, which represents the distance from the property to the street, has a high percentage of missing values, 18% in "train" and 16% in "test". Now we should check if data is missing by random or not using the Little test. However, this test is implemented in the LittleMCAR function, which accepts data frames with at most 50 variables. Both train and test datasets are too large, so we will have to make a decision with other tools.
# A quick look at the summary of LotFrontage in both datasets shows there isn't any house with a value of 0 for this variable. However, in the real world there exist houses whose entrance is right next to the street, with no separation from it. Hence, we deduce that missing values correspond to a distance of 0 and we impute LotFrontage like so.
# Assumption 5. BsmtHalfBath is numerical but it can only be 0, 1 or 2. Moreover, only 6% of values are positive, so we'll transform it to a categorical.
length(which(train$BsmtHalfBath > 0))/dim(train)[1]*100
length(which(test$BsmtHalfBath > 0))/dim(test)[1]*100
# Assumption 6. Similarly, KitchenAbvGr can only be 0, 1, 2 or 3 and only 5% is different than 1.
length(which(train$KitchenAbvGr != 1))/dim(train)[1]*100
length(which(test$KitchenAbvGr != 1))/dim(test)[1]*100
# Assumption 7. Most values of EnclosedPorch, X3SsnPorch, ScreenPorch are 0. Below we can see the percentage of each variable in train and test. Hence, we will transform them to a binary variable that tells whether the dwelling has a porch of the corresponding kind.
length(which(train$EnclosedPorch > 0))/dim(train)[1]*100
length(which(test$EnclosedPorch > 0))/dim(test)[1]*100
length(which(train$X3SsnPorch > 0))/dim(train)[1]*100
length(which(test$X3SsnPorch > 0))/dim(test)[1]*100
length(which(train$ScreenPorch > 0))/dim(train)[1]*100
length(which(test$ScreenPorch > 0))/dim(test)[1]*100
# Assumption 8. MiscVal has only a 4% of values different to 0. However, converting it to binary would be redundant, because wether a dwelling has a miscellaneous feature or not can already be studied with the MiscFeature categorical variable. Hence, we have decided to remove this variable from both train and test, storing its values in a new R variable in case we need it for further study.
length(which(train$MiscVal > 0))/dim(train)[1]*100
length(which(test$MiscVal > 0))/dim(test)[1]*100
# Assumption 9. We'll transform all the categorical variables we found previously into Factor.
# Assumption 10. Missings in categorical variables Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature correspond to a new level, so they cannot be imputed. Instead, we create this new level and declare it as all missings for each variable. Some of these missing values could be really missing and hence should me imputed with other methods. However, the rest of variables have at most 1% of missings, so we can assume that just a few records will be imputed wrongly. We have found this information by doing an automatic profile generated using the SmartEDA library, which can be found in the EDA section.
# Assumption 11. The MSSubClass, OverallQual and OverallCond, although are categorical variables, they are presented as integer. So we need to transform it into factor.
# Assumption 12. Checking the set of categories, we found that "Exterior2nd" has a record of "Brk Cmn", which does not match with the data description "BrkComm". So we rename it (in order to match with "Exterior1st")
# Assumption 13. Month will be transformed into factor and renamed to abbreviated names, while other dates as year will remain numerical.
# Note that the numerical variables we have converted to categorical were exactly those whose IQR was 0, since finding their univariate outliers would have been very complicated.
# Chunk 6
# Assumption 1
test <- test %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
test$PoolArea = as.factor(test$PoolArea)
train <- train %>%
mutate(PoolArea = ifelse(PoolArea > 0, "Yes", "No"))
train$PoolArea = as.factor(train$PoolArea)
# Assumption 2
test <- test %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
test$LowQualFinSF = as.factor(test$LowQualFinSF)
train <- train %>%
mutate(LowQualFinSF = ifelse(LowQualFinSF > 0, "Yes", "No"))
train$LowQualFinSF = as.factor(train$LowQualFinSF)
# Assumption 3
test <- test %>%
mutate(BsmtFinSF2 = ifelse(BsmtFinSF2 > 0, "Yes", "No"))
test$BsmtFinSF2 = as.factor(test$BsmtFinSF2)
train <- train %>%
mutate(BsmtFinSF2 = ifelse(BsmtFinSF2 > 0, "Yes", "No"))
train$BsmtFinSF2 = as.factor(train$BsmtFinSF2)
# Assumption 4
percent_miss <- function(data) {
return (length(which(is.na(data)))/length(data)*100)
}
percent_miss(train$LotFrontage)
percent_miss(test$LotFrontage)
summary(train$LotFrontage)
summary(test$LotFrontage)
lltrain <- which(is.na(train$LotFrontage))
lltest <- which(is.na(test$LotFrontage))
train$LotFrontage[lltrain] <- 0
test$LotFrontage[lltest] <- 0
# Assumption 5
train$BsmtHalfBath <- as.factor(train$BsmtHalfBath)
test$BsmtHalfBath <- as.factor(test$BsmtHalfBath)
# Assumption 6
train$KitchenAbvGr <- as.factor(train$KitchenAbvGr)
test$KitchenAbvGr <- as.factor(test$KitchenAbvGr)
levels(test$KitchenAbvGr) = c(levels(test$KitchenAbvGr),"3")
# Assumption 7
test <- test %>%
mutate(EnclosedPorch = ifelse(EnclosedPorch > 0, "Yes", "No"))
test$EnclosedPorch = as.factor(test$EnclosedPorch)
train <- train %>%
mutate(EnclosedPorch = ifelse(EnclosedPorch > 0, "Yes", "No"))
train$EnclosedPorch = as.factor(train$EnclosedPorch)
test <- test %>%
mutate(X3SsnPorch = ifelse(X3SsnPorch > 0, "Yes", "No"))
test$X3SsnPorch = as.factor(test$X3SsnPorch)
train <- train %>%
mutate(X3SsnPorch = ifelse(X3SsnPorch > 0, "Yes", "No"))
train$X3SsnPorch = as.factor(train$X3SsnPorch)
test <- test %>%
mutate(ScreenPorch = ifelse(ScreenPorch > 0, "Yes", "No"))
test$ScreenPorch = as.factor(test$ScreenPorch)
train <- train %>%
mutate(ScreenPorch = ifelse(ScreenPorch > 0, "Yes", "No"))
train$ScreenPorch = as.factor(train$ScreenPorch)
# Assumption 8
miscVal_train <- train$MiscVal
miscVal_test <- test$MiscVal
train$MiscVal <- NULL
test$MiscVal <- NULL
# Assumption 9
test <- test %>%
mutate_if(is.character, as.factor)
train <- train %>%
mutate_if(is.character, as.factor)
# Assumption 10
# Alley, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, PoolQC, Fence, MiscFeature
levels(train$Alley) <- c(levels(train$Alley), "NAlley")
train$Alley[which(is.na(train$Alley))] <- "NAlley"
levels(test$Alley) <- c(levels(test$Alley), "NAlley")
test$Alley[which(is.na(test$Alley))] <- "NAlley"
levels(train$BsmtQual) <- c(levels(train$BsmtQual), "NBsmt")
train$BsmtQual[which(is.na(train$BsmtQual))] <- "NBsmt"
levels(test$BsmtQual) <- c(levels(test$BsmtQual), "NBsmt")
test$BsmtQual[which(is.na(test$BsmtQual))] <- "NBsmt"
levels(train$BsmtCond) <- c(levels(train$BsmtCond), "NBsmt")
train$BsmtCond[which(is.na(train$BsmtCond))] <- "NBsmt"
levels(test$BsmtCond) <- c(levels(test$BsmtCond), "NBsmt")
test$BsmtCond[which(is.na(test$BsmtCond))] <- "NBsmt"
levels(train$BsmtExposure) <- c(levels(train$BsmtExposure), "NBsmt")
train$BsmtExposure[which(is.na(train$BsmtExposure))] <- "NBsmt"
levels(test$BsmtExposure) <- c(levels(test$BsmtExposure), "NBsmt")
test$BsmtExposure[which(is.na(test$BsmtExposure))] <- "NBsmt"
levels(train$BsmtFinType1) <- c(levels(train$BsmtFinType1), "NBsmt")
train$BsmtFinType1[which(is.na(train$BsmtFinType1))] <- "NBsmt"
levels(test$BsmtFinType1) <- c(levels(test$BsmtFinType1), "NBsmt")
test$BsmtFinType1[which(is.na(test$BsmtFinType1))] <- "NBsmt"
levels(train$BsmtFinType2) <- c(levels(train$BsmtFinType2), "NBsmt")
train$BsmtFinType2[which(is.na(train$BsmtFinType2))] <- "NBsmt"
levels(test$BsmtFinType2) <- c(levels(test$BsmtFinType2), "NBsmt")
test$BsmtFinType2[which(is.na(test$BsmtFinType2))] <- "NBsmt"
levels(train$FireplaceQu) <- c(levels(train$FireplaceQu), "NFp")
train$FireplaceQu[which(is.na(train$FireplaceQu))] <- "NFp"
levels(test$FireplaceQu) <- c(levels(test$FireplaceQu), "NFp")
test$FireplaceQu[which(is.na(test$FireplaceQu))] <- "NFp"
levels(train$GarageType) <- c(levels(train$GarageType), "NGar")
train$GarageType[which(is.na(train$GarageType))] <- "NGar"
levels(test$GarageType) <- c(levels(test$GarageType), "NGar")
test$GarageType[which(is.na(test$GarageType))] <- "NGar"
levels(train$GarageFinish) <- c(levels(train$GarageFinish), "NGar")
train$GarageFinish[which(is.na(train$GarageFinish))] <- "NGar"
levels(test$GarageFinish) <- c(levels(test$GarageFinish), "NGar")
test$GarageFinish[which(is.na(test$GarageFinish))] <- "NGar"
levels(train$GarageQual) <- c(levels(train$GarageQual), "NGar")
train$GarageQual[which(is.na(train$GarageQual))] <- "NGar"
levels(test$GarageQual) <- c(levels(test$GarageQual), "NGar")
test$GarageQual[which(is.na(test$GarageQual))] <- "NGar"
levels(train$GarageCond) <- c(levels(train$GarageCond), "NGar")
train$GarageCond[which(is.na(train$GarageCond))] <- "NGar"
levels(test$GarageCond) <- c(levels(test$GarageCond), "NGar")
test$GarageCond[which(is.na(test$GarageCond))] <- "NGar"
levels(train$PoolQC) <- c(levels(train$PoolQC), "NPool")
train$PoolQC[which(is.na(train$PoolQC))] <- "NPool"
levels(test) <- c(levels(test$PoolQC), "NPool")
test$PoolQC[which(is.na(test$PoolQC))] <- "NPool"
levels(train$Fence) <- c(levels(train$Fence), "NFen")
train$Fence[which(is.na(train$Fence))] <- "NFen"
levels(test$Fence) <- c(levels(test$Fence), "NFen")
test$Fence[which(is.na(test$Fence))] <- "NFen"
levels(train$MiscFeature) <- c(levels(train$MiscFeature), "N")
train$MiscFeature[which(is.na(train$MiscFeature))] <- "N"
levels(test$MiscFeature) <- c(levels(test$MiscFeature), "N")
test$MiscFeature[which(is.na(test$MiscFeature))] <- "N"
# Assumption 11
test$MSSubClass = as.factor(test$MSSubClass)
test$OverallQual = as.factor(test$OverallQual)
test$OverallCond = as.factor(test$OverallCond)
train$MSSubClass = as.factor(train$MSSubClass)
train$OverallQual = as.factor(train$OverallQual)
train$OverallCond = as.factor(train$OverallCond)
# Assumption 12
names(test)[names(test) == "Brk Cmn"] <- "BrkComm"
# Assumption 13.
test$MoSold = month.name[test$MoSold]
test$MoSold = as.factor(test$MoSold)
train$MoSold = month.name[train$MoSold]
train$MoSold = as.factor(train$MoSold)
summary(test)
# Chunk 7
# Find numerical, categorical and date variables
id_num_val = which(sapply(test, is.numeric)==TRUE)
# We won't analyze the id variable
id_num_val = as.numeric(id_num_val)[-1]; id_num_val
id_cat_val = which(sapply(test, is.factor)==TRUE)
id_cat_val = as.numeric(id_cat_val); id_cat_val
id_date_val = c(20,21,60,77,78)
# Now, the categorical variable are below
Categorical_val = c("MSSubClass","MSZoning","Street","Alley","LotShape","LandContour","Utilities","LotConfig","LandSlope","Neighborhood","Condition1","Condition2","BldgType","HouseStyle","OverallQual","OverallCond","RoofStyle","RoofMatl","Exterior1st","Exterior2nd","MasVnrType","ExterQual","ExterCond","Foundation","BsmtQual","BsmtCond","BsmtExposure","BsmtFinType1","BsmtFinType2","BsmtFinSF2","Heating","HeatingQC","CentralAir","Electrical","LowQualFinSF","BsmtHalfBath","KitchenAbvGr","KitchenQual","Functional","FireplaceQu","GarageType","GarageFinish","GarageQual","GarageCond","PavedDrive","EnclosedPorch","X3SsnPorch","ScreenPorch","PoolArea","PoolQC","Fence","MiscFeature","SaleType","SaleCondition","MoSold")
# The numerical variables except the target are
Numerical_val = c("LotFrontage","LotArea","YearBuilt","YearRemodAdd","MasVnrArea","BsmtFinSF1","BsmtUnfSF","TotalBsmtSF","X1stFlrSF","X2ndFlrSF","GrLivArea","BsmtFullBath","FullBath","HalfBath","BedroomAbvGr","TotRmsAbvGrd","Fireplaces","GarageYrBlt","GarageCars","GarageArea","WoodDeckSF","OpenPorchSF","YrSold")
train_num = select(train, Numerical_val); summary(train_num)
train_cat = select(train, Categorical_val); summary(train_cat)
# Chunk 8
# Here we'll check all the Univariate outliers, and highlight them. In the next section, we'll do it but for multiple outliers. If a record appears in Uni or Mult outlier analysis, then we'll remove it.
severe_out = Boxplot(train_num); severe_out
# There are 26 numerical variables, so we will automatize this step by erasing all severe univariate outliers and setting them to NA. After validating the results, we will imputate them with the rest of missing values. There will be one exception, SalePrice. Since it is the target, we can't impute it, so we will remove the whole records with severe outliers instead.
severe_outliers <- function(data) {
ss <- summary(data)
# Upper/lower severe thresholds
utso <- as.numeric(ss[5]+3*(ss[5]-ss[2]))
ltso <- as.numeric(ss[2]-3*(ss[5]-ss[2]))
return (which((data>utso)|(data<ltso)))
}
par(mfrow=c(1,2))
for (var in id_num_val) {
train[severe_outliers(train[,var]),var] <- NA
Boxplot(train[,var], ylab = var, main = "Train")
test[severe_outliers(test[,var]),var] <- NA
Boxplot(test[,var], ylab = var, main = "Test")
}
par(mfrow=c(1,1))
train = train[-severe_outliers(train$SalePrice),]
Boxplot(train$SalePrice)
# Chunk 9
percent_miss(train$GarageYrBlt)
percent_miss(test$GarageYrBlt)
percent_miss(train$MasVnrArea)
percent_miss(test$MasVnrArea)
percent_miss(train$OpenPorchSF)
percent_miss(test$OpenPorchSF)
# Chunk 10
# In this section we'll Performance PCA to impute the missing data for numerical variables
res.PCA = imputePCA (train[,id_num_val])   # impute numeric variables
str (res.PCA)
str(res.PCA$completeObs)
# We can observe that the missing values are imputed.
train_impute = data.frame(res.PCA$completeObs)
# Chunk 11
# Check if the imputation was successful or not
before_imputation = summary(train[,id_num_val])
after_imputation = summary(train_impute)
label = c('Before imputation', 'After imputation')
for (x in c(1,2,5,6,8,9,11,15,16,18,20,21,22)) {
d = data.frame(A = train[,id_num_val][x], B = train_impute[,x])
b = boxplot(d, names=label, main = names(train[,id_num_val][x]));
b
}
# Chunk 12
# In this section we'll Performance PCA to impute the missing data for numerical variables
res.PCA.test = imputePCA (test[,id_num_val])   # impute numeric variables
str (res.PCA.test)
str(res.PCA.test$completeObs)
# We can observe that the missing values are imputed.
test_impute = data.frame(res.PCA.test$completeObs)
# Chunk 13
# Check if the imputation was successful or not
before_imputation_test = summary(test[,id_num_val])
after_imputation_test = summary(test_impute)
label = c('Before imputation', 'After imputation')
for (x in c(1,2,5,6,8,9,11,15,16,18,20,21,22)) {
d = data.frame(A = test[,id_num_val][x], B = test_impute[,x])
b = boxplot(d, names=label, main = names(test[,id_num_val][x]));
b
}
# Chunk 14
# In the multivariate outlier, as this is stochastic, we cannot plot all the features. After several attempts, the best combination is below
id_num_val
id_num_val_not_corr = c(1, 2, 4, 6, 7, 11, 17,
18, 20)
# The analysis gives that all values from classical Mahalanobis distance
# are NA, as a consequence it is only possible to plot robust Mahalanobis distance.
# According to this distance there are 436 outliers (30 % of all data), however,
# none of them is considered an outlier by the classical Mahalanobis distance.
res.mout <- Moutlier(train_impute[,id_num_val_not_corr], quantile = 0.95, plot= FALSE)
par(mfrow=c(1,2))
plot(res.mout$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))
outliers = which(res.mout$md>res.mout$cutoff & res.mout$rd > res.mout$cutoff)
length(outliers)
# Chunk 15
res.mout.test <- Moutlier(test_impute[,id_num_val_not_corr], quantile = 0.95, plot= FALSE)
par(mfrow=c(1,2))
plot(res.mout.test$md, col="lightblue", pch = 19, main = 'Detection of multivariable
outliers', xlab= 'Observation',
ylab ='Traditional Mahalanobis distance ')
abline(h = res.mout.test$cutoff, col = "red", lwd = 5, lty = 2)
plot(res.mout.test$rd, col="lightblue", pch = 19, xlab= 'Observation',
ylab ='Robust Mahalanobis distance ')
abline(h = res.mout.test$cutoff, col = "red", lwd = 5, lty = 2)
par(mfrow=c(1,1))
outliers.test = which(res.mout.test$md>res.mout.test$cutoff & res.mout.test$rd > res.mout.test$cutoff)
length(outliers.test)
# Chunk 16
# Use condes() method to determine the correlation between categorical feature and the target SalePrice.
res.con = condes(train, 80)
res.con$quanti
res.con$quali
res.con$category
# Chunk 17
cor.test(train$YrSold, train$SalePrice)
res.con$quali[1:10,]
# load DataExplorer library
#use create_report
library(DataExplorer)
create_report(train)
# load DataExplorer library
#use create_report
library(DataExplorer)
create_report(train, output_file = "train.html")
create_report(train_impute, output_file = "train_imputed.html")
create_report(test, output_file = "test.html")
create_report(test_impute, output_file = "test_imputed.html")
#library(SmartEDA)
#ExpReport(train, op_file = 'train_before_imputation_SMARTEDA.html')
#ExpReport(test, op_file = 'test_before_imputation_SMARTEDA.html')
#ExpReport(train_impute, op_file = 'train_before_imputation_SMARTEDA.html')
# load DataExplorer library
#use create_report
library(DataExplorer)
create_report(train, output_file = "train.html")
create_report(train_impute, output_file = "train_imputed.html")
create_report(test, output_file = "test.html")
create_report(test_impute, output_file = "test_imputed.html")
#library(SmartEDA)
#ExpReport(train, op_file = 'train_before_imputation_SMARTEDA.html')
#ExpReport(test, op_file = 'test_before_imputation_SMARTEDA.html')
#ExpReport(train_impute, op_file = 'train_before_imputation_SMARTEDA.html')
